{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Genetic programming: symbolic regresion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import gp\n",
    "import plotting as pl\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "from itertools import product\n",
    "from random import seed\n",
    "from math import sin, cos, pi, exp, sqrt, log\n",
    "import time\n",
    "from datetime import datetime\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set of problems\n",
    "problems = ({'func' : lambda x: exp(abs(x))*sin(2*pi*x), 'interval' : np.array(list(product(np.linspace(-3,3,200)))), 'name' : 'e^|x|*sin(x)'},)\n",
    "def generate_dataset(inputs, func):\n",
    "    outputs = np.zeros_like(inputs[:, 0])\n",
    "    for i, t in enumerate(inputs):\n",
    "        outputs[i] = func(*t)\n",
    "    return np.column_stack([inputs, np.vstack(outputs)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_filename(name):\n",
    "    return datetime.now().strftime(f'%d_%m_%H_%M_{name}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed() # set seed here to reproduce same conditions\n",
    "d = generate_dataset(problems[0]['interval'], problems[0]['func'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('1. experiment: 25 runs, 5 min per run')\n",
    "runs = 25\n",
    "gp.set_params(generations=np.inf, evaluations_limit=np.inf, time_limit=5*60)\n",
    "x = gp.GeneticProgram(d)\n",
    "p = gp.SLFitnessPredictorManager(x, d.shape[0])\n",
    "gp.perform_runs(runs, x, None, file=get_filename('exp1_no_pred'))\n",
    "gp.perform_runs(runs, x, p, file=get_filename('exp1_pred'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('2. experiment: 25 runs, 3.37e7 evals per run')\n",
    "runs = 25\n",
    "gp.set_params(generations=np.inf, evaluations_limit=3.37e7, time_limit=np.inf)\n",
    "x = gp.GeneticProgram(d)\n",
    "p = gp.SLFitnessPredictorManager(x, d.shape[0])\n",
    "gp.perform_runs(runs, x, None, file=get_filename('exp2_no_pred'))\n",
    "gp.perform_runs(runs, x, p, file=get_filename('exp2_pred'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1 = np.load('12_12_19_58_test_no_pred.npz', allow_pickle=True)\n",
    "f2 = np.load('12_12_20_00_test_pred.npz', allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res1, res2 = f1['results'], f2['results']\n",
    "print(res1[0].keys())\n",
    "xs1, ys1 = utils.prepare_data([res1[i]['times'] for i in range(runs)], \\\n",
    "                              [res1[i]['best_of_run_fitnesses'] for i in range(runs)])\n",
    "xs2, ys2 = utils.prepare_data([res2[i]['times'] for i in range(runs)], \\\n",
    "                              [res2[i]['best_of_run_fitnesses'] for i in range(runs)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "means1 = np.mean(ys1, axis=0)\n",
    "means2 = np.mean(ys2, axis=0)\n",
    "\n",
    "means2[means2 > 1e5] = 10\n",
    "\n",
    "plt.plot(xs1, means1, color='blue', label='using all test cases')\n",
    "plt.plot(xs2, means2, color='red', label='using fitness predictors')\n",
    "plt.title('with vs without using fitness predictor')\n",
    "plt.xlabel('time')\n",
    "plt.ylabel('fitness')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best1 = np.argmax([res1[i]['best_of_run_fitnesses'][-1] for i in range(runs)])\n",
    "best2 = np.argmax([res2[i]['best_of_run_fitnesses'][-1] for i in range(runs)])\n",
    "plt.plot(d[:, 0], res1[best1]['best_solutions'][-1])\n",
    "plt.title(f'best solution with {res1[best1][\"best_of_run_fitnesses\"][-1]} fitness')\n",
    "plt.scatter(d[:, 0], d[:, 1])\n",
    "plt.show()\n",
    "plt.plot(d[:, 0], res1[best2]['best_solutions'][-1])\n",
    "plt.title(f'best solution with {res1[best2][\"best_of_run_fitnesses\"][-1]} fitness')\n",
    "          \n",
    "plt.scatter(d[:, 0], d[:, 1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## this will be removed in the future"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# run gp on problems with parameters specified above\n",
    "seed() # set seed here to reproduce same conditions\n",
    "start = time.time()\n",
    "\n",
    "for problem in problems:\n",
    "    d = generate_dataset(problem['interval'], problem['func'])\n",
    "    x = gp.GeneticProgram(d)\n",
    "    fp_manager = gp.SLFitnessPredictorManager(x, d.shape[0])\n",
    "    \n",
    "    x_values = []\n",
    "    used_predictors = []\n",
    "    fitnesses = []\n",
    "    best_ever_fitness = np.inf\n",
    "    best_ever = None\n",
    "    worse_solutions = []\n",
    "    \n",
    "    for i in range(runs):\n",
    "        res = x.run_evolution(fp_manager=fp_manager, verbose=False)\n",
    "        if res['best_of_run_exact_fitness'] < best_ever_fitness:\n",
    "            best_ever_fitness = res['best_of_run_exact_fitness']\n",
    "            best_ever = res['best']\n",
    "    \n",
    "        x_values.append(res['test_cases_evaluations'])\n",
    "        used_predictors.append(res['used_predictors'])\n",
    "        fitnesses.append(res['best_of_run_fitnesses'])\n",
    "        worse_solutions.append(res['worse_solution'])\n",
    "        \n",
    "    \n",
    "    # analyze situations where worse solution was prefered\n",
    "    for i, sl in enumerate(worse_solutions):\n",
    "        for j, s in enumerate(sl):\n",
    "            # todo using res here doesnt make any sense at all and for this to work on multiple runs this has to be rewritten\n",
    "            predictor_idx = np.where(x_values[i] == s)[0][0]\n",
    "            plt.scatter(np.ones_like(used_predictors[i][0])*-1, used_predictors[i][predictor_idx - 1])\n",
    "            plt.scatter(np.ones_like(used_predictors[i][0]), used_predictors[i][predictor_idx])\n",
    "            plt.title(f'run: {i} change between {x_values[i][predictor_idx - 1]} -> {x_values[i][predictor_idx]}\\n solution with {fitnesses[i][predictor_idx]} was prefered to {fitnesses[i][predictor_idx - 1]}')\n",
    "            plt.savefig(f'{i}_{j}_0')\n",
    "            plt.clf()\n",
    "            plt.subplot(121)\n",
    "            plt.title('previous')\n",
    "            pl.target_with_predictor(d, res['best_solutions'][predictor_idx - 1],used_predictors[i][predictor_idx - 1])\n",
    "            plt.subplot(122)\n",
    "            plt.title('new')\n",
    "            pl.target_with_predictor(d, res['best_solutions'][predictor_idx],used_predictors[i][predictor_idx])\n",
    "            #print(x.fitness(res['best_solutions'][predictor_idx-1]), x.fitness(res['best_solutions'][predictor_idx]))\n",
    "            \n",
    "            plt.savefig(f'{i}_{j}_1')\n",
    "            plt.clf()\n",
    "    \n",
    "    # select widest range\n",
    "    x_values = x_values[np.argmax([len(v) for v in x_values])]\n",
    "    \n",
    "    # fill shorter runs with last values (in case some run converged or something...)\n",
    "    for i in range(len(fitnesses)):\n",
    "        while len(used_predictors[i]) < len(x_values): \n",
    "            used_predictors[i] = np.vstack([used_predictors[i], used_predictors[i][-1]])\n",
    "        while len(fitnesses[i]) < len(x_values):\n",
    "            fitnesses[i] = np.append(fitnesses[i], fitnesses[i][-1])\n",
    "    \n",
    "    # samples to use \n",
    "    idxs = np.linspace(0, len(x_values)-1, 15, dtype=np.int32)\n",
    "\n",
    "    used_predictors = np.concatenate(used_predictors)\n",
    "    \n",
    "\n",
    "    #used_predictors = np.concatenate(used_predictors).flatten()\n",
    "    used_predictors = d[:, 0][used_predictors.flatten()]\n",
    "    fitnesses = np.vstack(fitnesses)\n",
    "    \n",
    "    # plot histogram\n",
    "    vals, bins = np.histogram(used_predictors, bins=d.shape[0])\n",
    "    # make bins same size as graph of the function\n",
    "    vals = vals * ((max(d[:, 1]) - min(d[:, 1])) / max(vals))\n",
    "    plt.bar(bins[:-1], vals, bottom=min(d[:, 1]), align='edge', alpha=0.75, width=(d[-1, 0]-d[0, 0]) / len(vals))\n",
    "    plt.plot(d[:, 0], d[:, 1], label='target function')\n",
    "    plt.title('histogram of used test cases')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "    #plot fitness\n",
    "    avg_fitnesses = np.mean(fitnesses, axis=0)\n",
    "    errors = np.std(fitnesses, axis=0)\n",
    "    plt.errorbar(x_values[idxs], avg_fitnesses[idxs], yerr=errors[idxs], capsize=7)\n",
    "    \n",
    "    #for s in worse_solutions:\n",
    "    #    for ss in s:\n",
    "    #        plt.axvline(x=ss, ymin=0, ymax=max(avg_fitnesses), color='r', linestyle='dashed')\n",
    "    plt.title('exact fitness of best solution averaged over runs')\n",
    "    plt.show()\n",
    "\n",
    "    print(f'best solution of the run:')\n",
    "    pl.plot_solution_and_target(best_ever, problem['name'] , d)\n",
    "    end = time.time()\n",
    "    print(f'execution time: {end - start}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
